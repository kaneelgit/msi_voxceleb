{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multimodal_classification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyObhr6RYwp46gF98VSAcTkA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaneelgit/msi_voxceleb/blob/main/Multimodal_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvnvL5rfm9HY"
      },
      "source": [
        "<font size = '6'> <center>**Multimodal Classification**</center></font>\n",
        "\n",
        "Here I am using a multimodal approach. I will be using both video and audio data for gender classification. The videos feature extraction will be done by a 3D CNN model as before and the audio classification will be done by a 2D classification model as before. \n",
        "\n",
        "VoxCeleb dataset - https://www.robots.ox.ac.uk/~vgg/data/voxceleb/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hUyLehvmr4m",
        "outputId": "b4fd2cd1-425e-41e3-808d-64055908524b"
      },
      "source": [
        "#import libraries\n",
        "from urllib.request import urlopen\n",
        "from zipfile import ZipFile\n",
        "\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2     # for capturing videos\n",
        "import math\n",
        "import random\n",
        "\n",
        "import moviepy.editor as mp\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import IPython\n",
        "import librosa\n",
        "\n",
        "from scipy.io import wavfile\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imageio: 'ffmpeg-linux64-v3.3.1' was not found on your computer; downloading it now.\n",
            "Try 1. Download from https://github.com/imageio/imageio-binaries/raw/master/ffmpeg/ffmpeg-linux64-v3.3.1 (43.8 MB)\n",
            "Downloading: 8192/45929032 bytes (0.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2334720/45929032 bytes (5.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b5292032/45929032 bytes (11.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b8396800/45929032 bytes (18.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11673600/45929032 bytes (25.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15024128/45929032 bytes (32.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b18235392/45929032 bytes (39.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b21389312/45929032 bytes (46.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b24485888/45929032 bytes (53.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b27533312/45929032 bytes (59.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b30760960/45929032 bytes (67.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b33816576/45929032 bytes (73.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b37076992/45929032 bytes (80.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b40148992/45929032 bytes (87.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b43499520/45929032 bytes (94.7%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45929032/45929032 bytes (100.0%)\n",
            "  Done\n",
            "File saved as /root/.imageio/ffmpeg/ffmpeg-linux64-v3.3.1.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAYmTkGOoAmR"
      },
      "source": [
        "# **Download and unzip the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQI9vcEWoEUZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddd6240f-6b2f-40ff-abd3-276bed8ba70f"
      },
      "source": [
        "#get url\n",
        "!wget \"https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox2_test_mp4.zip\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-17 19:32:51--  https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox2_test_mp4.zip\n",
            "Resolving thor.robots.ox.ac.uk (thor.robots.ox.ac.uk)... 129.67.95.98\n",
            "Connecting to thor.robots.ox.ac.uk (thor.robots.ox.ac.uk)|129.67.95.98|:443... connected.\n",
            "HTTP request sent, awaiting response... "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOF5ENJXocup"
      },
      "source": [
        "!unzip vox2_test_mp4.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XG7Ew3wIohXL"
      },
      "source": [
        "# **Organizing the data**\n",
        "\n",
        "In the cells below I am collecting all the video paths in a list. I will be then using these videos to extract the audios and their spectrograms to feed in to the neural network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWY6HS5qolDv"
      },
      "source": [
        "#get video paths\n",
        "vid_paths = []\n",
        "\n",
        "for path, directories, files in os.walk('/content/mp4/'):\n",
        "\n",
        "  for file in files:\n",
        "\n",
        "    vid_paths.append(str(path) + '/' + str(file))\n",
        "\n",
        "#number of videos available\n",
        "print('Number of videos available: ', len(vid_paths))\n",
        "\n",
        "#shuffle video paths. I have used a random seed of 4 to shuffle.\n",
        "random.seed(4)\n",
        "random.shuffle(vid_paths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hv9uwkDDo3M2"
      },
      "source": [
        "# **Upload the CSV file and clean meta data**\n",
        "\n",
        "\n",
        "The csv file with meta data is in my github repository. (https://github.com/kaneelgit/msi_voxceleb). You have to upload the csv file to colab before running the following cells."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eG7zdCMeo7M7"
      },
      "source": [
        "#some functions to clean the csv file\n",
        "#del spaces from the ids and gender\n",
        "def del_spaces(string):\n",
        "  \n",
        "  string = string.replace(' ', '')\n",
        "\n",
        "  return string\n",
        "\n",
        "#upload csv file from github before running\n",
        "#open csv file\n",
        "df = pd.read_csv('/content/vox2_meta.csv')\n",
        "df.head(5)\n",
        "\n",
        "#clean the dataset\n",
        "\n",
        "#apply the function to get rid of spaces\n",
        "df['VoxCeleb2 ID'] = df['VoxCeleb2 ID'].apply(del_spaces)\n",
        "df['VGGFace2 ID'] = df['VGGFace2 ID'].apply(del_spaces)\n",
        "df['Gender'] = df['Gender'].apply(del_spaces)\n",
        "df['Set'] = df['Set'].apply(del_spaces)\n",
        "\n",
        "\n",
        "#get the test data\n",
        "df2 = df[df['Set'] == 'test']\n",
        "df2['Gender'] = df2['Gender'].astype('category')\n",
        "\n",
        "df2.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XRcch4RpFwr"
      },
      "source": [
        "# **Create a Labels List**\n",
        "\n",
        "Here I am going through the ids of all the video files and picking the gender from the CSV file. Note that there are double the size of male videos compared to female. So I will be taking every other video file to balance the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xfWWVIhpMmF"
      },
      "source": [
        "#iterate through the vid_paths get the id of the person and get if the person is female or male. If the person is male its a 1 and female its a 0\n",
        "labels = []\n",
        "\n",
        "#get only half of the male videos.\n",
        "count = 0\n",
        "video_files = []\n",
        "\n",
        "#iterate\n",
        "for path in vid_paths:\n",
        "  \n",
        "  #get id number\n",
        "  id_str =  path[13:20]\n",
        "\n",
        "  #get if the subject is male or female from the csv\n",
        "  gender = df2.loc[df2['VoxCeleb2 ID'] == str(id_str)]['Gender'].values[0]\n",
        "\n",
        "  if gender == 'm':\n",
        "    if count % 2 == 0:\n",
        "      labels.append(1)\n",
        "      video_files.append(path)\n",
        "    count += 1\n",
        "  \n",
        "  else:\n",
        "    labels.append(0)\n",
        "    video_files.append(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D3f6wPvpv6a"
      },
      "source": [
        "# **Create a data generator**\n",
        "\n",
        "Here I am creating a data generator to feed audio and video data to the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WO6wcSj7qQPo"
      },
      "source": [
        "#functions to extract the spectrogram and video to array\n",
        "\n",
        "def vid_to_array(path):\n",
        "  \n",
        "  r = np.zeros([224, 224, 15])\n",
        "  g = np.zeros([224, 224, 15])\n",
        "  b = np.zeros([224, 224, 15])\n",
        "\n",
        "  #start count\n",
        "  count = 0\n",
        "  idx = 0\n",
        "\n",
        "  #video capture\n",
        "  cap = cv2.VideoCapture(path)\n",
        "\n",
        "  while(cap.isOpened()):\n",
        "      #frameId = cap.get(1) #current frame number\n",
        "      ret, frame = cap.read()\n",
        "\n",
        "      if (count + 1) % 5 == 0:\n",
        "        r[:, :, idx] = frame[:, :, 0]/255\n",
        "        g[:, :, idx] = frame[:, :, 1]/255\n",
        "        b[:, :, idx] = frame[:, :, 2]/255\n",
        "\n",
        "      if count == 74:\n",
        "        break\n",
        "      count += 1\n",
        "  cap.release()\n",
        "\n",
        "  return np.stack([r, g, b], axis = 3)\n",
        "\n",
        "#convert from audio to a mel diagram\n",
        "def aud_to_mel(path):\n",
        "  \n",
        "  #get video from path\n",
        "  video = mp.VideoFileClip(path)\n",
        "  video = video.subclip(0, 3)\n",
        "\n",
        "  #get the audio\n",
        "  x = video.audio.to_soundarray()\n",
        "\n",
        "  #get audio track to array\n",
        "  #x = aud.to_soundarray()\n",
        "\n",
        "  #sampling rate\n",
        "  fs = 44100\n",
        "  \n",
        "  #channel 1\n",
        "  ch1 = librosa.feature.melspectrogram(x[:, 0], sr = fs)\n",
        "  mel_ch1 = librosa.amplitude_to_db(ch1, ref = np.min)\n",
        "\n",
        "  #channel 2\n",
        "  # ch2 = librosa.feature.melspectrogram(x[:, 1], sr = fs)\n",
        "  # mel_ch2 = librosa.amplitude_to_db(ch2, ref = np.min)\n",
        "  \n",
        "  return mel_ch1.reshape(mel_ch1.shape[0], mel_ch1.shape[1], 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfhTajmJtSbJ"
      },
      "source": [
        "#create a data generator\n",
        "def data_generator(paths, labels, batch_size = 16):\n",
        "\n",
        "  while True:\n",
        "\n",
        "    #get the number of batches\n",
        "    number_of_batches = len(paths) // batch_size\n",
        "\n",
        "    #set up video batches and label batches\n",
        "    batches = [paths[i : i + batch_size] for i in range(0, len(paths), batch_size)]\n",
        "    label_batches = [labels[i : i + batch_size] for i in range(0, len(paths), batch_size)]\n",
        "\n",
        "    for b in range(number_of_batches):\n",
        "      \n",
        "      #create array to store video batch\n",
        "      x_batch_video = np.zeros([batch_size, 224, 224, 15, 3])\n",
        "      \n",
        "      #create array to store audio batch\n",
        "      x_batch_audio = np.zeros([batch_size, 128, 259, 1])\n",
        "      \n",
        "      #labels\n",
        "      y_batch = np.zeros([batch_size, 1])\n",
        "\n",
        "      #get video batch and label batch\n",
        "      batch = batches[b]\n",
        "      label_batch = label_batches[b]\n",
        "\n",
        "      #iterate through the batch path and get the array \n",
        "      for i, p in enumerate(batch):\n",
        "        \n",
        "        #get video array and store the result\n",
        "        vid_array = vid_to_array(p)\n",
        "        x_batch_video[i, :] = vid_array\n",
        "\n",
        "        #get audio array and store the result\n",
        "        audio_array = aud_to_mel(p)\n",
        "        x_batch_audio[i, :] = audio_array\n",
        "\n",
        "        #y batch\n",
        "        y_batch[i, :] = label_batch[i]\n",
        "    \n",
        "      yield {'video': x_batch_video, 'audio': x_batch_audio}, {'class': y_batch}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6GtzLB8pZ2y"
      },
      "source": [
        "# **Train/validation split**\n",
        "\n",
        "Here I have manually seperated the train and validation set because I ran into memory issues in colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LNwY5pupik2"
      },
      "source": [
        "#split to train and validation set\n",
        "train_idx = int(np.floor(len(labels) * 0.9))\n",
        "\n",
        "#split the train and test data accordingly\n",
        "train_paths = video_files[:train_idx]\n",
        "train_labels = labels[:train_idx]\n",
        "\n",
        "#split the test data\n",
        "val_paths = video_files[train_idx:]\n",
        "val_labels = labels[train_idx:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqpmSskVps-c"
      },
      "source": [
        "#validation set\n",
        "validation_set_vid = np.zeros([150, 224, 224, 15, 3])\n",
        "validation_set_aud = np.zeros([150, 128, 259, 1])\n",
        "validation_labels = np.zeros([150, 1])\n",
        "\n",
        "#select 100 random indexes from the validation set\n",
        "#rand_val_idx = np.random.choice(np.arange(len(val_paths)), 150)\n",
        "rand_val_idx = np.arange(len(val_paths[:150]))\n",
        "\n",
        "for i, idx in enumerate(rand_val_idx):\n",
        "\n",
        "  #vid path\n",
        "  path = val_paths[idx]\n",
        "  label = val_labels[idx]\n",
        "\n",
        "  validation_set_vid[i, :] = vid_to_array(path)\n",
        "  validation_set_aud[i, :] = aud_to_mel(path)\n",
        "  validation_labels[i, :] = label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhDuKGZzjm2m"
      },
      "source": [
        "#create a train set\n",
        "training_set_vid = np.zeros([800, 224, 224, 15, 3])\n",
        "training_set_aud = np.zeros([800, 128, 259, 1])\n",
        "training_labels = np.zeros([800, 1])\n",
        "\n",
        "#select 1000 files from the train files\n",
        "rand_train_idx = np.arange(len(train_paths[:800]))\n",
        "\n",
        "for i, idx in enumerate(rand_train_idx):\n",
        "\n",
        "  #aud path\n",
        "  path = train_paths[idx]\n",
        "  label = train_labels[idx]\n",
        "\n",
        "  training_set_vid[i, :] = vid_to_array(path)\n",
        "  training_set_aud[i, :] = aud_to_mel(path)\n",
        "  training_labels[i, :] = label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeiHOwvWw5d_"
      },
      "source": [
        "# **Multimodal Architecture**\n",
        "\n",
        "Here I create a multimodal architecture to use both audio and video data for classification\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5Rv-u6ow4dT"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import scipy.misc\n",
        "from tensorflow.keras.applications.resnet_v2 import ResNet50V2\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet_v2 import preprocess_input, decode_predictions\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Input, Add, Dense, Average, Activation, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D, Dropout, Concatenate\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.initializers import random_uniform, glorot_uniform, constant, identity\n",
        "from tensorflow.python.framework.ops import EagerTensor\n",
        "from tensorflow.python.keras.utils.vis_utils import plot_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_ngiSV4yEjz"
      },
      "source": [
        "#3d cnn for video\n",
        "def cnn_3dmodel(x):\n",
        "    \n",
        "    inputs = x\n",
        "    x = tf.keras.layers.Conv3D(16, kernel_size = (3, 3, 1),activation = 'relu')(inputs)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.MaxPool3D(pool_size = (2, 2, 2))(x)\n",
        "    \n",
        "    x = tf.keras.layers.Conv3D(32, kernel_size = (3, 3, 1), activation = 'relu')(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.MaxPool3D(pool_size = (2, 2, 2))(x)\n",
        "    \n",
        "    x = tf.keras.layers.Conv3D(64, kernel_size = (3, 3, 1), activation = 'relu')(x) \n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.MaxPool3D(pool_size = (2, 2, 1))(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "#video inp\n",
        "input_video_tmp = np.zeros([16, 224, 224, 15, 3])\n",
        "\n",
        "#creat video layers\n",
        "input_vid = Input(input_video_tmp[0, :].shape, name = 'video')\n",
        "out_vid = cnn_3dmodel(input_vid)\n",
        "out_vid = Flatten()(out_vid)\n",
        "out_vid = Dropout(0.5)(out_vid)\n",
        "out_vid = Dense(1024, activation = 'relu')(out_vid)\n",
        "out_vid = Dense(128, activation = 'relu')(out_vid)\n",
        "output_video = Dense(1, activation = 'sigmoid', name = 'video_class')(out_vid)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oemzMBwz86k"
      },
      "source": [
        "#2d cnn for audio\n",
        "def cnn_2dmodel(x):\n",
        "    \n",
        "    inputs = x\n",
        "    x = tf.keras.layers.Conv2D(16, kernel_size = (3, 3),activation = 'relu')(inputs)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.MaxPool2D(pool_size = (2, 2))(x)\n",
        "    \n",
        "    x = tf.keras.layers.Conv2D(32, kernel_size = (3, 3), activation = 'relu')(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.MaxPool2D(pool_size = (2, 2))(x)\n",
        "    \n",
        "    x = tf.keras.layers.Conv2D(64, kernel_size = (3, 3), activation = 'relu')(x) \n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.MaxPool2D(pool_size = (2, 2))(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "#create audio layers\n",
        "input_audio_tmp = np.zeros([16, 128, 259, 1])\n",
        "input_aud = Input(input_audio_tmp[0, :].shape, name = 'audio')\n",
        "out_aud = cnn_2dmodel(input_aud)\n",
        "out_aud = Flatten()(out_aud)\n",
        "out_aud = Dropout(0.5)(out_aud)\n",
        "out_aud = Dense(1024, activation = 'relu')(out_aud)\n",
        "out_aud = Dropout(0.8)(out_aud)\n",
        "out_aud = Dense(128, activation = 'relu')(out_aud)\n",
        "output_audio = Dense(1, activation = 'sigmoid', name = 'audio_class')(out_aud)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QD9mumKU3KAW"
      },
      "source": [
        "#create the combined model\n",
        "combined_out = tf.keras.layers.Concatenate(axis=1)([output_video, output_audio])\n",
        "combined_out = Dense(1, activation = 'sigmoid', name = 'class')(combined_out)\n",
        "combined_model = Model([input_vid, input_aud], combined_out)\n",
        "\n",
        "#plot the combined model\n",
        "plot_model(combined_model, show_shapes = True, show_layer_names = True, to_file = 'test_model.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nb5PCla04S71"
      },
      "source": [
        "#compile the model\n",
        "opt = tf.optimizers.Adam(0.001)\n",
        "\n",
        "#learning rate scheduler\n",
        "def lr_schedule(epoch, lr):\n",
        "    \n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        \n",
        "        lr *= 0.2\n",
        "    \n",
        "    return max(lr, 3e-7)\n",
        "\n",
        "opt = tf.keras.optimizers.SGD(\n",
        "    learning_rate=0.01, momentum=0.0, nesterov=False, name='SGD')\n",
        "\n",
        "#compile model\n",
        "combined_model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = 'accuracy')\n",
        "\n",
        "#callbacks\n",
        "#create a callback to stop the model when loss hit plateur\n",
        "cb = tf.keras.callbacks.EarlyStopping(monitor = 'loss', patience = 5, mode = 'auto')\n",
        "\n",
        "cb_lr = tf.keras.callbacks.LearningRateScheduler(lr_schedule)\n",
        "\n",
        "#fit model\n",
        "history = combined_model.fit(data_generator(train_paths, train_labels, batch_size = 8), validation_data = ([validation_set_vid, validation_set_aud], validation_labels),\\\n",
        "                    steps_per_epoch = 100, epochs = 10, callbacks = [cb, cb_lr])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PffOVPtGbU2i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}